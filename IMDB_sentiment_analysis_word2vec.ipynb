{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB sentiment analysis with Word2Vec (Kaggle) \n",
    "[task link](https://www.kaggle.com/c/word2vec-nlp-tutorial/data)\n",
    "\n",
    "> Use Google's Word2Vec for movie reviews\n",
    "\n",
    "Data Set: <br>\n",
    "* The labeled data set consists of 50,000 IMDB movie reviews, specially selected for sentiment analysis. <br>\n",
    "* The sentiment of reviews is binary, meaning the IMDB rating < 5 results in a sentiment score of 0, and rating >=7 have a sentiment score of 1.<br>\n",
    "* No individual movie has more than 30 reviews. The 25,000 review labeled training set does not include any of the same movies as the 25,000 review test set. <br>\n",
    "* In addition, there are another 50,000 IMDB reviews provided without any rating labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: \n",
    "\n",
    "Method 1. Predict labeded Data by BOW, tf-idf and MultinomialNB.\n",
    "\n",
    "    a. check basic info of dataset\n",
    "    b. data cleaning with stopwords (use NLTK)\n",
    "    c. Build model by CountVectorizer and MultinomialNB (use GridsearhCV and Pipline)\n",
    "    d. Build model by TfidfVectorizer and MultinomialNB (use GridsearhCV and Pipline)\n",
    "    e. Kaggle data submission\n",
    "\n",
    "Method 2. Use Word2Vec and Tree-based models to do prediction.\n",
    "\n",
    "    a. Train Word2Vec model by unlabeled data\n",
    "    b. Turn labeled reviews data into vectors by trained Word2Vec model\n",
    "    c. Fit the vector data by GBDT and do prediction.\n",
    "    d. Fit the vector data by XGboost and do prediction.\n",
    "    e. Kaggle data submission\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kaggle submission result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![subsmission result](https://github.com/vivianchang2019/IMDB_word2vec_kaggle/blob/master/result/word2vec_reviews.JPG?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Predict labeded Data by BOW, tf-idf and MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('data/IMDB/labeledTrainData.tsv', delimiter = '\\t')\n",
    "df_test = pd.read_csv('data/IMDB/testData.tsv', delimiter = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. check basic info of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 25000 entries, 0 to 24999\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   id         25000 non-null  object\n",
      " 1   sentiment  25000 non-null  int64 \n",
      " 2   review     25000 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 586.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df_train.info() # no missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5814_8</td>\n",
       "      <td>1</td>\n",
       "      <td>With all this stuff going down at the moment w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2381_9</td>\n",
       "      <td>1</td>\n",
       "      <td>\\The Classic War of the Worlds\\\" by Timothy Hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7759_3</td>\n",
       "      <td>0</td>\n",
       "      <td>The film starts with a manager (Nicholas Bell)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3630_4</td>\n",
       "      <td>0</td>\n",
       "      <td>It must be assumed that those who praised this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9495_8</td>\n",
       "      <td>1</td>\n",
       "      <td>Superbly trashy and wondrously unpretentious 8...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  sentiment                                             review\n",
       "0  5814_8          1  With all this stuff going down at the moment w...\n",
       "1  2381_9          1  \\The Classic War of the Worlds\\\" by Timothy Hi...\n",
       "2  7759_3          0  The film starts with a manager (Nicholas Bell)...\n",
       "3  3630_4          0  It must be assumed that those who praised this...\n",
       "4  9495_8          1  Superbly trashy and wondrously unpretentious 8..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12311_10</td>\n",
       "      <td>Naturally in a film who's main themes are of m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8348_2</td>\n",
       "      <td>This movie is a disaster within a disaster fil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5828_4</td>\n",
       "      <td>All in all, this is a movie for kids. We saw i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7186_2</td>\n",
       "      <td>Afraid of the Dark left me with the impression...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12128_7</td>\n",
       "      <td>A very accurate depiction of small time mob li...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                             review\n",
       "0  12311_10  Naturally in a film who's main themes are of m...\n",
       "1    8348_2  This movie is a disaster within a disaster fil...\n",
       "2    5828_4  All in all, this is a movie for kids. We saw i...\n",
       "3    7186_2  Afraid of the Dark left me with the impression...\n",
       "4   12128_7  A very accurate depiction of small time mob li..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. data cleaning with stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data cleaning\n",
    "\n",
    "def review_to_text(review):\n",
    "    # remove HTML tag\n",
    "    raw_text=BeautifulSoup(review, 'html').get_text()\n",
    "    # remove non English character\n",
    "    letters = re.sub('[^a-zA-Z]', ' ', raw_text)\n",
    "    words = letters.lower().split()\n",
    "    # remove stopwords\n",
    "    stopw_words = set(stopwords.words('English'))\n",
    "    words = ' '.join([w for w in words if w not in stopw_words])\n",
    "\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train['review'].apply(review_to_text)\n",
    "X_test = df_train['review'].apply(review_to_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    stuff going moment mj started listening music ...\n",
       "1    classic war worlds timothy hines entertaining ...\n",
       "2    film starts manager nicholas bell giving welco...\n",
       "3    must assumed praised film greatest filmed oper...\n",
       "4    superbly trashy wondrously unpretentious explo...\n",
       "Name: review, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df_train['sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Build model by CountVectorizer and MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[CountVectorizer document](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms text to vectors. \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pipline to build two classifiers with CountVectorizer \n",
    "pip_count = Pipeline([('count_vec', CountVectorizer(analyzer='word')),\n",
    "                     ('mnb', MultinomialNB())])             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set hyperparameters for GridSearch\n",
    "params_count = {'count_vec__binary':[True, False],\n",
    "                'count_vec__ngram_range': [(1, 1), (1, 2)],\n",
    "                'mnb__alpha': [0.1, 1.0, 10.0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 12 candidates, totalling 48 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  48 out of  48 | elapsed:  1.8min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=4,\n",
       "             estimator=Pipeline(steps=[('count_vec', CountVectorizer()),\n",
       "                                       ('mnb', MultinomialNB())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'count_vec__binary': [True, False],\n",
       "                         'count_vec__ngram_range': [(1, 1), (1, 2)],\n",
       "                         'mnb__alpha': [0.1, 1.0, 10.0]},\n",
       "             verbose=1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run GridSearch for CountVectorizer\n",
    "\n",
    "gs_count = GridSearchCV(pip_count, params_count, cv=4, n_jobs=-1, verbose=1)\n",
    "gs_count.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.88216\n",
      "{'count_vec__binary': True, 'count_vec__ngram_range': (1, 2), 'mnb__alpha': 1.0}\n"
     ]
    }
   ],
   "source": [
    "print(gs_count.best_score_)\n",
    "print(gs_count.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the data with model and best parameter\n",
    "count_y_predict = gs_count.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d. Build model by TfidfVectorizer and MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip_tfidf = Pipeline([('tfidf_vec', TfidfVectorizer(analyzer='word')),\n",
    "                     ('mnb', MultinomialNB())])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_tfidf = {'tfidf_vec__binary':[True, False],\n",
    "                'tfidf_vec__ngram_range': [(1, 1), (1, 2)],\n",
    "                'mnb__alpha': [0.1, 1.0, 10.0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 12 candidates, totalling 48 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  48 out of  48 | elapsed:  2.0min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=4,\n",
       "             estimator=Pipeline(steps=[('tfidf_vec', TfidfVectorizer()),\n",
       "                                       ('mnb', MultinomialNB())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'mnb__alpha': [0.1, 1.0, 10.0],\n",
       "                         'tfidf_vec__binary': [True, False],\n",
       "                         'tfidf_vec__ngram_range': [(1, 1), (1, 2)]},\n",
       "             verbose=1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run GridSearch for TfidfVectorizer\n",
    "gs_tfidf = GridSearchCV(pip_tfidf, params_tfidf, cv=4, n_jobs=-1, verbose=1)\n",
    "gs_tfidf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.88712\n",
      "{'mnb__alpha': 0.1, 'tfidf_vec__binary': True, 'tfidf_vec__ngram_range': (1, 2)}\n"
     ]
    }
   ],
   "source": [
    "print(gs_tfidf.best_score_)\n",
    "print(gs_tfidf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_y_predict = gs_tfidf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### e. Kaggle data submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_count = pd.DataFrame({'id': df_test['id'], 'sentiment': count_y_predict})\n",
    "submission_tfidf = pd.DataFrame({'id': df_test['id'], 'sentiment': tfidf_y_predict})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission_count.to_csv('result/submission_count.csv', index = False)\n",
    "# submission_tfidf.to_csv('result/submission_tfidf.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# b. Use Word2Vec and Tree-based models to do prediction\n",
    "\n",
    "#### a. Train Word2Vec model by unlabeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_train = pd.read_csv('data/IMDB/unlabeledTrainData.tsv', delimiter = '\\t', quoting = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"9999_0\"</td>\n",
       "      <td>\"Watching Time Chasers, it obvious that it was...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"45057_0\"</td>\n",
       "      <td>\"I saw this film about 20 years ago and rememb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"15561_0\"</td>\n",
       "      <td>\"Minor Spoilers&lt;br /&gt;&lt;br /&gt;In New York, Joan B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"7161_0\"</td>\n",
       "      <td>\"I went to see this film with a great deal of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"43971_0\"</td>\n",
       "      <td>\"Yes, I agree with everyone on this site this ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                             review\n",
       "0   \"9999_0\"  \"Watching Time Chasers, it obvious that it was...\n",
       "1  \"45057_0\"  \"I saw this film about 20 years ago and rememb...\n",
       "2  \"15561_0\"  \"Minor Spoilers<br /><br />In New York, Joan B...\n",
       "3   \"7161_0\"  \"I went to see this film with a great deal of ...\n",
       "4  \"43971_0\"  \"Yes, I agree with everyone on this site this ..."
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabeled_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# nltk.download('punkt')\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to divides a text into a list of sentences, and do string cleaning by each sentence\n",
    "\n",
    "\n",
    "def review_to_text_word2vec(review):\n",
    "    # remove HTML tag\n",
    "    try:\n",
    "        raw_text = BeautifulSoup(review, 'html').get_text()\n",
    "    except:\n",
    "        raw_text = review\n",
    "    # remove non English character\n",
    "    letters = re.sub('[^a-zA-Z]', ' ', raw_text)\n",
    "    words = letters.lower().split()\n",
    "    # remove stopwords\n",
    "    stopw_words = set(stopwords.words('English'))\n",
    "    words = [w for w in words if w not in stopw_words]\n",
    "    return words\n",
    "\n",
    "\n",
    "def review_to_sentences(review, tokenizer):\n",
    "    # remove HTML tag\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence)> 0:\n",
    "            sentences.append(review_to_text_word2vec(raw_sentence))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VivianChang\\anaconda3\\envs\\NLP\\lib\\site-packages\\bs4\\__init__.py:332: MarkupResemblesLocatorWarning: \".\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n",
      "C:\\Users\\VivianChang\\anaconda3\\envs\\NLP\\lib\\site-packages\\bs4\\__init__.py:417: MarkupResemblesLocatorWarning: \"http://www.archive.org/details/LovefromaStranger\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "C:\\Users\\VivianChang\\anaconda3\\envs\\NLP\\lib\\site-packages\\bs4\\__init__.py:417: MarkupResemblesLocatorWarning: \"http://www.loosechangeguide.com/LooseChangeGuide.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "C:\\Users\\VivianChang\\anaconda3\\envs\\NLP\\lib\\site-packages\\bs4\\__init__.py:332: MarkupResemblesLocatorWarning: \"... ...\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n",
      "C:\\Users\\VivianChang\\anaconda3\\envs\\NLP\\lib\\site-packages\\bs4\\__init__.py:332: MarkupResemblesLocatorWarning: \"...\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n",
      "C:\\Users\\VivianChang\\anaconda3\\envs\\NLP\\lib\\site-packages\\bs4\\__init__.py:332: MarkupResemblesLocatorWarning: \"....\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n",
      "C:\\Users\\VivianChang\\anaconda3\\envs\\NLP\\lib\\site-packages\\bs4\\__init__.py:417: MarkupResemblesLocatorWarning: \"http://www.msnbc.msn.com/id/4972055/site/newsweek/\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "C:\\Users\\VivianChang\\anaconda3\\envs\\NLP\\lib\\site-packages\\bs4\\__init__.py:332: MarkupResemblesLocatorWarning: \"..\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n",
      "C:\\Users\\VivianChang\\anaconda3\\envs\\NLP\\lib\\site-packages\\bs4\\__init__.py:417: MarkupResemblesLocatorWarning: \"http://www.youtube.com/watch?v=a0KSqelmgN8\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "C:\\Users\\VivianChang\\anaconda3\\envs\\NLP\\lib\\site-packages\\bs4\\__init__.py:332: MarkupResemblesLocatorWarning: \".. .\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n",
      "C:\\Users\\VivianChang\\anaconda3\\envs\\NLP\\lib\\site-packages\\bs4\\__init__.py:417: MarkupResemblesLocatorWarning: \"http://jake-weird.blogspot.com/2007/08/beneath.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# prepare data for Word2vec training\n",
    "\n",
    "corpora = []\n",
    "for review in unlabeled_train['review']:\n",
    "    corpora += review_to_sentences(review, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Word2Vec document](https://radimrehurek.com/gensim/models/word2vec.html) <br>\n",
    "[Word2Vec explaination](https://allenlu2007.wordpress.com/2018/05/18/nlp-%E8%87%AA%E7%84%B6%E8%AA%9E%E8%A8%80%E8%99%95%E7%90%86-word2vec-%E5%B7%A5%E5%85%B7%E7%AE%B1%E5%92%8C-skip-gram/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train Word2Vec model\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "num_features = 300\n",
    "\n",
    "model = Word2Vec(corpora, size = num_features, workers = 4, min_count = 20, window = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('model/word2vec_300features') # save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lady', 0.5992120504379272),\n",
       " ('woman', 0.5843505859375),\n",
       " ('men', 0.5464239716529846),\n",
       " ('lad', 0.45248642563819885),\n",
       " ('payback', 0.43589454889297485),\n",
       " ('gentleman', 0.43261802196502686),\n",
       " ('person', 0.43119335174560547),\n",
       " ('monk', 0.429901123046875),\n",
       " ('mans', 0.41871994733810425),\n",
       " ('angus', 0.41664761304855347)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the training result\n",
    "model.wv.most_similar(\"man\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Turn labeled reviews data into vectors by trained Word2Vec mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to average all of the word vectors in a given　paragraph\n",
    "\n",
    "def makeFeatureVec(words, model, num_features):\n",
    "\n",
    "    # Pre-initialize an empty numpy array\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    nwords = 0.\n",
    "\n",
    "    # a set contains the words in the model's vocabulary\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "\n",
    "    # create feature vector of words if a word in index2word_set\n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1.\n",
    "            featureVec = np.add(featureVec,model.wv.__getitem__([word]))\n",
    "    featureVec = np.divide(featureVec,nwords)\n",
    "    \n",
    "    return featureVec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the average feature vector for each review \n",
    "\n",
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    counter = 0\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "\n",
    "    for review in reviews:\n",
    "        reviewFeatureVecs[counter] = makeFeatureVec(review, model, num_features)\n",
    "        counter = counter + 1\n",
    "        \n",
    "    return reviewFeatureVecs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfer training and testing sets to be vectors\n",
    "\n",
    "clean_train_reviews = []\n",
    "for review in df_train[\"review\"]:\n",
    "    clean_train_reviews.append(review_to_text_word2vec(review))\n",
    "\n",
    "trainDataVecs = getAvgFeatureVecs(clean_train_reviews, model, num_features )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_test_reviews = []\n",
    "for review in df_test[\"review\"]:\n",
    "    clean_test_reviews.append(review_to_text_word2vec(review))\n",
    "\n",
    "testDataVecs = getAvgFeatureVecs(clean_test_reviews, model, num_features )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Fit the vector data by GBDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc = GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_gbc = {'n_estimators':[10, 100, 500],\n",
    "              'learning_rate': [0.01, 0.1, 1.0],\n",
    "              'max_depth': [2, 3, 4]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_gbc = GridSearchCV(gbc, params_gbc, cv=4, n_jobs=-1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 27 candidates, totalling 108 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed: 45.8min\n",
      "[Parallel(n_jobs=-1)]: Done 108 out of 108 | elapsed: 193.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=4, estimator=GradientBoostingClassifier(), n_jobs=-1,\n",
       "             param_grid={'learning_rate': [0.01, 0.1, 1.0],\n",
       "                         'max_depth': [2, 3, 4],\n",
       "                         'n_estimators': [10, 100, 500]},\n",
       "             verbose=1)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_gbc.fit(trainDataVecs, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8565600000000001\n",
      "{'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 500}\n"
     ]
    }
   ],
   "source": [
    "print(gs_gbc.best_score_)\n",
    "print(gs_gbc.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_gbc = gs_gbc.predict(testDataVecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_w2v_gbc = pd.DataFrame({'id': df_test['id'], 'sentiment': result_gbc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission_w2v_gbc.to_csv('result/submission_w2v_gbc.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d. Fit the vector data by XGboost for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = xgb.XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_xgb = {'n_estimators':[10, 100, 500],\n",
    "              'learning_rate': [0.01, 0.1, 1.0],\n",
    "              'max_depth': [2, 3, 4]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_xgb = GridSearchCV(xgb, params_xgb, cv=4, n_jobs=-1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 27 candidates, totalling 108 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed: 12.9min\n",
      "[Parallel(n_jobs=-1)]: Done 108 out of 108 | elapsed: 51.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=4,\n",
       "             estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                     colsample_bylevel=None,\n",
       "                                     colsample_bynode=None,\n",
       "                                     colsample_bytree=None, gamma=None,\n",
       "                                     gpu_id=None, importance_type='gain',\n",
       "                                     interaction_constraints=None,\n",
       "                                     learning_rate=None, max_delta_step=None,\n",
       "                                     max_depth=None, min_child_weight=None,\n",
       "                                     missing=nan, monotone_constraints=None,\n",
       "                                     n_estimators=100, n_jobs=None,\n",
       "                                     num_parallel_tree=None, random_state=None,\n",
       "                                     reg_alpha=None, reg_lambda=None,\n",
       "                                     scale_pos_weight=None, subsample=None,\n",
       "                                     tree_method=None, validate_parameters=None,\n",
       "                                     verbosity=None),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'learning_rate': [0.01, 0.1, 1.0],\n",
       "                         'max_depth': [2, 3, 4],\n",
       "                         'n_estimators': [10, 100, 500]},\n",
       "             verbose=1)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_xgb.fit(trainDataVecs, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8560800000000001\n",
      "{'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 500}\n"
     ]
    }
   ],
   "source": [
    "print(gs_xgb.best_score_)\n",
    "print(gs_xgb.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### e. Kaggle data submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_xgb = gs_xgb.predict(testDataVecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_w2v_xgb = pd.DataFrame({'id': df_test['id'], 'sentiment': result_xgb})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission_w2v_xgb.to_csv('result/submission_w2v_xgb.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
